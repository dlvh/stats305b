{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_JIQE4w9xbB"
      },
      "source": [
        "# HW1: Logistic Regression\n",
        "\n",
        "This class is about models and algorithms for discrete data. This homework will have all 3 ingredients:\n",
        "* **Data**: the results from all college football games in the 2023 season\n",
        "* **Model**: The *Bradely-Terry* model for predicting the winners of football game. The Bradley-Terry model is just logistic regression.\n",
        "* **Algorithm**: We will implement two ways of fitting logistic regression: gradient descent and Newton's method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oi2v2m5yCJE9"
      },
      "source": [
        "## The Bradley-Terry Model\n",
        "\n",
        "In the Bradley-Terry Model, we give team $k$ a team-effect $\\beta_k$. Basically, higher $\\beta_k$ (relatively speaking), means that team $k$ is a better team.\n",
        "The Bradley-Terry model formalizes this intution by modeling the log odds of team $k$ beating team $k'$ by the difference in their team effects, $\\beta_k - \\beta_{k'}$.\n",
        "\n",
        "Let $i = 1,\\ldots, n$ index games, and let $h(i) \\in \\{1,\\ldots,K\\}$ and $a(i) \\in \\{1,\\ldots,K\\}$ denote the indices of the home and away teams, respectively.\n",
        "Let $Y_i \\in \\{0,1\\}$ denote whether the home team won.\n",
        "Under the Bradley-Terry model,\n",
        "\\begin{equation*}\n",
        "  Y_i \\sim \\mathrm{Bern}\\big(\\sigma(\\beta_{h(i)} - \\beta_{a(i)}) \\big),\n",
        "\\end{equation*}\n",
        "where $\\sigma(\\cdot)$ is the sigmoid function. We can view this model as a logistic regression model with covariates $x_i \\in \\mathbb{R}^K$ where,\n",
        "\\begin{align*}\n",
        "x_{i,k} &=\n",
        "\\begin{cases}\n",
        "+1 &\\text{if } h(i) = k \\\\\n",
        "-1 &\\text{if } a(i) = k \\\\\n",
        "0 &\\text{o.w.},\n",
        "\\end{cases}\n",
        "\\end{align*}\n",
        "and parameters $\\beta \\in \\mathbb{R}^K$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toIIF0ej-a7I"
      },
      "source": [
        "## Data\n",
        "\n",
        "We use the results of college football games in the fall 2023 season, which are available from the course github page and loaded for you below.\n",
        "\n",
        "The data comes as a list of the outcomes of individual games. You'll need to wrangle the data to get it into a format that you can feed into the Bradley-Terry model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "qvTw_232nr-v"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "WIYCdEBqnvJG"
      },
      "outputs": [],
      "source": [
        "allgames = pd.read_csv(\"https://raw.githubusercontent.com/slinderman/stats305b/winter2025/data/01_allgames.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_00WB7kaeDi"
      },
      "source": [
        "## Problem 0: Preprocessing\n",
        "\n",
        "Preprocess the data to drop games with nan scores, construct the covariate matrix $X$, construct the response vector $y$, and do any other preprocessing you find useful."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# drop games with nan scores\n",
        "# store the results in a new dataframe called df\n",
        "df = allgames.dropna(subset=['Home Points', 'Away Points'])\n",
        "# to avoid bias, drop all the games where the points are tied.\n",
        "df = df[df[\"Away Points\"] != df[\"Home Points\"]]\n",
        "\n",
        "# in Bradley-Terry model, constructing the response vector y\n",
        "# the covariate matrix X is very sparse. we can one column for each team,\n",
        "# and in each row, value equals 1 if the team is the home team, and equals -1 if the team is the away team,\n",
        "# and equals 0 for all other teams.\n",
        "# therefore, there is only one possible covariate matrix.\n",
        "points_difference = df[\"Home Points\"] - df[\"Away Points\"]\n",
        "y = (points_difference > 0)*1 # consider the home team won if the home team had higher points in the game\n",
        "teams = sorted(set(df[\"Away Team\"]).union(set(df[\"Home Team\"])))\n",
        "X_df = pd.DataFrame(0, index=df.index, columns=teams)\n",
        "for i, game in df.iterrows():\n",
        "  home_team = game[\"Home Team\"]\n",
        "  away_team = game[\"Away Team\"]\n",
        "  X_df.loc[i, home_team] = 1\n",
        "  X_df.loc[i, away_team] = -1\n",
        "# convert into tensors\n",
        "#type(y), type(X_df)\n",
        "y = torch.tensor(y.values, dtype=torch.float32)\n",
        "X = torch.tensor(X_df.values, dtype=torch.float32)\n",
        "y.shape, X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwtDjkd4a_cx",
        "outputId": "454ecb29-ea7f-421d-bccd-7a18662c59bb"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3574]), torch.Size([3574, 703]))"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Jk2AnZxfaeDi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e017a7b9-15d0-406f-d656-05e7a6816c4d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1.,  ..., 1., 0., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjUJOkAWHWD0"
      },
      "source": [
        "## Problem 1: Loss function\n",
        "\n",
        "Write a function to compute the loss, $L(\\beta)$ defined be\n",
        "\n",
        "\\begin{equation*}\n",
        "  L(\\beta) = -\\frac{1}{n} \\sum_{i=1}^n \\log p(y_i \\mid x_i; \\beta) + \\frac{\\gamma}{2} \\| \\beta \\|_2^2\n",
        "\\end{equation*}\n",
        "where $\\gamma$ is a hyperparameter that controls the strength of your $\\ell_2$ regularization.\n",
        "\n",
        "You may want to use the `torch.distributions.Bernoulli` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "WTaCXlvSHuxh"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "def compute_loss(beta, X, y, gamma):\n",
        "  average_log_likelihood = torch.distributions.Bernoulli(probs = torch.sigmoid(X @ beta)).log_prob(y).mean()\n",
        "  loss = -1 * average_log_likelihood + gamma / 2 * (torch.norm(beta, 2) ** 2) # add l2 regularization\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unit testing\n",
        "beta = torch.ones([X.shape[1], 1], dtype=torch.float32)\n",
        "gamma = 0\n",
        "compute_loss(beta, X, y, gamma)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdNEwOU9WTQd",
        "outputId": "70058f66-2b17-49c1-9729-6685d550fbe7"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.6931)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "beta"
      ],
      "metadata": {
        "id": "xmkOm6i_Yk9Q",
        "outputId": "a13855d8-f141-4e5f-f7e5-2094ec82c8f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.7576],\n",
              "        [0.2793],\n",
              "        [0.4031],\n",
              "        [0.7347],\n",
              "        [0.0293],\n",
              "        [0.7999],\n",
              "        [0.3971],\n",
              "        [0.7544],\n",
              "        [0.5695],\n",
              "        [0.4388],\n",
              "        [0.6387],\n",
              "        [0.5247],\n",
              "        [0.6826],\n",
              "        [0.3051],\n",
              "        [0.4635],\n",
              "        [0.4550],\n",
              "        [0.5725],\n",
              "        [0.4980],\n",
              "        [0.9371],\n",
              "        [0.6556],\n",
              "        [0.3138],\n",
              "        [0.1980],\n",
              "        [0.4162],\n",
              "        [0.2843],\n",
              "        [0.3398],\n",
              "        [0.5239],\n",
              "        [0.7981],\n",
              "        [0.7718],\n",
              "        [0.0112],\n",
              "        [0.8100],\n",
              "        [0.6397],\n",
              "        [0.9743],\n",
              "        [0.8300],\n",
              "        [0.0444],\n",
              "        [0.0246],\n",
              "        [0.2588],\n",
              "        [0.9391],\n",
              "        [0.4167],\n",
              "        [0.7140],\n",
              "        [0.2676],\n",
              "        [0.9906],\n",
              "        [0.2885],\n",
              "        [0.8750],\n",
              "        [0.5059],\n",
              "        [0.2366],\n",
              "        [0.7570],\n",
              "        [0.2346],\n",
              "        [0.6471],\n",
              "        [0.3556],\n",
              "        [0.4452],\n",
              "        [0.0193],\n",
              "        [0.2616],\n",
              "        [0.7713],\n",
              "        [0.3785],\n",
              "        [0.9980],\n",
              "        [0.9008],\n",
              "        [0.4766],\n",
              "        [0.1663],\n",
              "        [0.8045],\n",
              "        [0.6552],\n",
              "        [0.1768],\n",
              "        [0.8248],\n",
              "        [0.8036],\n",
              "        [0.9434],\n",
              "        [0.2197],\n",
              "        [0.4177],\n",
              "        [0.4903],\n",
              "        [0.5730],\n",
              "        [0.1205],\n",
              "        [0.1452],\n",
              "        [0.7720],\n",
              "        [0.3828],\n",
              "        [0.7442],\n",
              "        [0.5285],\n",
              "        [0.6642],\n",
              "        [0.6099],\n",
              "        [0.6818],\n",
              "        [0.7479],\n",
              "        [0.0369],\n",
              "        [0.7517],\n",
              "        [0.1484],\n",
              "        [0.1227],\n",
              "        [0.5304],\n",
              "        [0.4148],\n",
              "        [0.7937],\n",
              "        [0.2104],\n",
              "        [0.0555],\n",
              "        [0.8639],\n",
              "        [0.4259],\n",
              "        [0.7812],\n",
              "        [0.6607],\n",
              "        [0.1251],\n",
              "        [0.6004],\n",
              "        [0.6201],\n",
              "        [0.1652],\n",
              "        [0.2628],\n",
              "        [0.6705],\n",
              "        [0.5896],\n",
              "        [0.2873],\n",
              "        [0.3486],\n",
              "        [0.9579],\n",
              "        [0.4075],\n",
              "        [0.7819],\n",
              "        [0.7165],\n",
              "        [0.1768],\n",
              "        [0.0748],\n",
              "        [0.9799],\n",
              "        [0.5261],\n",
              "        [0.8427],\n",
              "        [0.6036],\n",
              "        [0.6608],\n",
              "        [0.8735],\n",
              "        [0.9741],\n",
              "        [0.1682],\n",
              "        [0.5625],\n",
              "        [0.8731],\n",
              "        [0.8622],\n",
              "        [0.8106],\n",
              "        [0.1381],\n",
              "        [0.1399],\n",
              "        [0.1976],\n",
              "        [0.5628],\n",
              "        [0.9983],\n",
              "        [0.1842],\n",
              "        [0.7664],\n",
              "        [0.2233],\n",
              "        [0.0299],\n",
              "        [0.3937],\n",
              "        [0.7881],\n",
              "        [0.9642],\n",
              "        [0.1895],\n",
              "        [0.6085],\n",
              "        [0.9314],\n",
              "        [0.8313],\n",
              "        [0.8116],\n",
              "        [0.8553],\n",
              "        [0.8163],\n",
              "        [0.6291],\n",
              "        [0.1581],\n",
              "        [0.0801],\n",
              "        [0.2709],\n",
              "        [0.4418],\n",
              "        [0.1935],\n",
              "        [0.6829],\n",
              "        [0.6547],\n",
              "        [0.3868],\n",
              "        [0.6922],\n",
              "        [0.6616],\n",
              "        [0.8053],\n",
              "        [0.8367],\n",
              "        [0.3307],\n",
              "        [0.9885],\n",
              "        [0.4422],\n",
              "        [0.4828],\n",
              "        [0.0281],\n",
              "        [0.1782],\n",
              "        [0.2079],\n",
              "        [0.2861],\n",
              "        [0.8555],\n",
              "        [0.3366],\n",
              "        [0.1264],\n",
              "        [0.6924],\n",
              "        [0.6601],\n",
              "        [0.8238],\n",
              "        [0.2413],\n",
              "        [0.6084],\n",
              "        [0.3180],\n",
              "        [0.3877],\n",
              "        [0.1015],\n",
              "        [0.2721],\n",
              "        [0.3469],\n",
              "        [0.7138],\n",
              "        [0.5913],\n",
              "        [0.6235],\n",
              "        [0.9991],\n",
              "        [0.9873],\n",
              "        [0.8410],\n",
              "        [0.5159],\n",
              "        [0.1541],\n",
              "        [0.8908],\n",
              "        [0.3750],\n",
              "        [0.4596],\n",
              "        [0.0693],\n",
              "        [0.4012],\n",
              "        [0.1775],\n",
              "        [0.9595],\n",
              "        [0.0677],\n",
              "        [0.1103],\n",
              "        [0.4830],\n",
              "        [0.2296],\n",
              "        [0.6789],\n",
              "        [0.3075],\n",
              "        [0.2652],\n",
              "        [0.5283],\n",
              "        [0.8619],\n",
              "        [0.1483],\n",
              "        [0.7348],\n",
              "        [0.8212],\n",
              "        [0.9891],\n",
              "        [0.1500],\n",
              "        [0.6211],\n",
              "        [0.1303],\n",
              "        [0.9269],\n",
              "        [0.3060],\n",
              "        [0.8012],\n",
              "        [0.5149],\n",
              "        [0.4611],\n",
              "        [0.4840],\n",
              "        [0.5850],\n",
              "        [0.7357],\n",
              "        [0.5802],\n",
              "        [0.6525],\n",
              "        [0.0502],\n",
              "        [0.8643],\n",
              "        [0.9359],\n",
              "        [0.9133],\n",
              "        [0.8696],\n",
              "        [0.1392],\n",
              "        [0.3146],\n",
              "        [0.9409],\n",
              "        [0.1192],\n",
              "        [0.9536],\n",
              "        [0.1068],\n",
              "        [0.1478],\n",
              "        [0.7444],\n",
              "        [0.1408],\n",
              "        [0.3854],\n",
              "        [0.8637],\n",
              "        [0.8960],\n",
              "        [0.9729],\n",
              "        [0.3985],\n",
              "        [0.1114],\n",
              "        [0.9923],\n",
              "        [0.3935],\n",
              "        [0.2943],\n",
              "        [0.6219],\n",
              "        [0.1503],\n",
              "        [0.8286],\n",
              "        [0.8134],\n",
              "        [0.1033],\n",
              "        [0.0893],\n",
              "        [0.4562],\n",
              "        [0.7100],\n",
              "        [0.4855],\n",
              "        [0.2465],\n",
              "        [0.5114],\n",
              "        [0.0300],\n",
              "        [0.1466],\n",
              "        [0.1672],\n",
              "        [0.9118],\n",
              "        [0.9408],\n",
              "        [0.3302],\n",
              "        [0.5224],\n",
              "        [0.7230],\n",
              "        [0.5599],\n",
              "        [0.2496],\n",
              "        [0.7884],\n",
              "        [0.8074],\n",
              "        [0.4710],\n",
              "        [0.4384],\n",
              "        [0.9544],\n",
              "        [0.9371],\n",
              "        [0.2165],\n",
              "        [0.9892],\n",
              "        [0.6237],\n",
              "        [0.1679],\n",
              "        [0.7737],\n",
              "        [0.1267],\n",
              "        [0.9620],\n",
              "        [0.1786],\n",
              "        [0.6414],\n",
              "        [0.6523],\n",
              "        [0.6189],\n",
              "        [0.9147],\n",
              "        [0.2923],\n",
              "        [0.2889],\n",
              "        [0.0667],\n",
              "        [0.4795],\n",
              "        [0.2631],\n",
              "        [0.5200],\n",
              "        [0.3975],\n",
              "        [0.6659],\n",
              "        [0.9325],\n",
              "        [0.6475],\n",
              "        [0.3389],\n",
              "        [0.2547],\n",
              "        [0.0639],\n",
              "        [0.9207],\n",
              "        [0.4053],\n",
              "        [0.6009],\n",
              "        [0.5185],\n",
              "        [0.1814],\n",
              "        [0.7816],\n",
              "        [0.7802],\n",
              "        [0.5252],\n",
              "        [0.2159],\n",
              "        [0.2875],\n",
              "        [0.5069],\n",
              "        [0.2143],\n",
              "        [0.2210],\n",
              "        [0.4254],\n",
              "        [0.3560],\n",
              "        [0.6227],\n",
              "        [0.3686],\n",
              "        [0.4337],\n",
              "        [0.3092],\n",
              "        [0.0431],\n",
              "        [0.9361],\n",
              "        [0.5954],\n",
              "        [0.9471],\n",
              "        [0.4194],\n",
              "        [0.3743],\n",
              "        [0.0299],\n",
              "        [0.3102],\n",
              "        [0.8279],\n",
              "        [0.0103],\n",
              "        [0.3251],\n",
              "        [0.8046],\n",
              "        [0.7882],\n",
              "        [0.6846],\n",
              "        [0.4010],\n",
              "        [0.4621],\n",
              "        [0.4050],\n",
              "        [0.3157],\n",
              "        [0.7087],\n",
              "        [0.3297],\n",
              "        [0.2191],\n",
              "        [0.6134],\n",
              "        [0.1386],\n",
              "        [0.3933],\n",
              "        [0.7555],\n",
              "        [0.6869],\n",
              "        [0.9257],\n",
              "        [0.1590],\n",
              "        [0.5043],\n",
              "        [0.3523],\n",
              "        [0.2203],\n",
              "        [0.2187],\n",
              "        [0.6260],\n",
              "        [0.5523],\n",
              "        [0.2448],\n",
              "        [0.1476],\n",
              "        [0.0857],\n",
              "        [0.9900],\n",
              "        [0.4148],\n",
              "        [0.3844],\n",
              "        [0.3161],\n",
              "        [0.2499],\n",
              "        [0.0427],\n",
              "        [0.2065],\n",
              "        [0.8056],\n",
              "        [0.6096],\n",
              "        [0.3236],\n",
              "        [0.3110],\n",
              "        [0.8194],\n",
              "        [0.8604],\n",
              "        [0.9817],\n",
              "        [0.6459],\n",
              "        [0.7416],\n",
              "        [0.4598],\n",
              "        [0.2077],\n",
              "        [0.0102],\n",
              "        [0.8045],\n",
              "        [0.4273],\n",
              "        [0.7054],\n",
              "        [0.5246],\n",
              "        [0.0333],\n",
              "        [0.2547],\n",
              "        [0.6407],\n",
              "        [0.3988],\n",
              "        [0.0172],\n",
              "        [0.6753],\n",
              "        [0.5192],\n",
              "        [0.3393],\n",
              "        [0.7020],\n",
              "        [0.6056],\n",
              "        [0.3156],\n",
              "        [0.9492],\n",
              "        [0.7508],\n",
              "        [0.5628],\n",
              "        [0.4040],\n",
              "        [0.0765],\n",
              "        [0.8860],\n",
              "        [0.3710],\n",
              "        [0.1769],\n",
              "        [0.8206],\n",
              "        [0.6776],\n",
              "        [0.6237],\n",
              "        [0.4009],\n",
              "        [0.3440],\n",
              "        [0.5848],\n",
              "        [0.8869],\n",
              "        [0.5716],\n",
              "        [0.2636],\n",
              "        [0.9967],\n",
              "        [0.5107],\n",
              "        [0.8953],\n",
              "        [0.6219],\n",
              "        [0.9482],\n",
              "        [0.7885],\n",
              "        [0.2451],\n",
              "        [0.8512],\n",
              "        [0.5032],\n",
              "        [0.0986],\n",
              "        [0.3952],\n",
              "        [0.0487],\n",
              "        [0.2409],\n",
              "        [0.1458],\n",
              "        [0.8420],\n",
              "        [0.3950],\n",
              "        [0.4136],\n",
              "        [0.7396],\n",
              "        [0.7035],\n",
              "        [0.1700],\n",
              "        [0.6843],\n",
              "        [0.7124],\n",
              "        [0.1024],\n",
              "        [0.9347],\n",
              "        [0.1958],\n",
              "        [0.3045],\n",
              "        [0.0882],\n",
              "        [0.9570],\n",
              "        [0.5952],\n",
              "        [0.9210],\n",
              "        [0.5327],\n",
              "        [0.7718],\n",
              "        [0.3404],\n",
              "        [0.1011],\n",
              "        [0.5803],\n",
              "        [0.4554],\n",
              "        [0.4495],\n",
              "        [0.2544],\n",
              "        [0.5104],\n",
              "        [0.6436],\n",
              "        [0.6804],\n",
              "        [0.0972],\n",
              "        [0.7416],\n",
              "        [0.5514],\n",
              "        [0.8449],\n",
              "        [0.8534],\n",
              "        [0.1062],\n",
              "        [0.9802],\n",
              "        [0.0083],\n",
              "        [0.7874],\n",
              "        [0.5352],\n",
              "        [0.7366],\n",
              "        [0.2296],\n",
              "        [0.8006],\n",
              "        [0.2526],\n",
              "        [0.0581],\n",
              "        [0.6675],\n",
              "        [0.7737],\n",
              "        [0.9956],\n",
              "        [0.4477],\n",
              "        [0.9810],\n",
              "        [0.8212],\n",
              "        [0.2520],\n",
              "        [0.1143],\n",
              "        [0.7804],\n",
              "        [0.2261],\n",
              "        [0.7293],\n",
              "        [0.0718],\n",
              "        [0.8648],\n",
              "        [0.6499],\n",
              "        [0.4425],\n",
              "        [0.7293],\n",
              "        [0.1440],\n",
              "        [0.3907],\n",
              "        [0.5049],\n",
              "        [0.2111],\n",
              "        [0.3722],\n",
              "        [0.2844],\n",
              "        [0.9765],\n",
              "        [0.9248],\n",
              "        [0.9312],\n",
              "        [0.9087],\n",
              "        [0.0211],\n",
              "        [0.6673],\n",
              "        [0.6843],\n",
              "        [0.1333],\n",
              "        [0.7027],\n",
              "        [0.2287],\n",
              "        [0.5795],\n",
              "        [0.6656],\n",
              "        [0.9314],\n",
              "        [0.2377],\n",
              "        [0.1066],\n",
              "        [0.9626],\n",
              "        [0.7099],\n",
              "        [0.7430],\n",
              "        [0.6800],\n",
              "        [0.3230],\n",
              "        [0.8008],\n",
              "        [0.0635],\n",
              "        [0.5696],\n",
              "        [0.7260],\n",
              "        [0.8416],\n",
              "        [0.2503],\n",
              "        [0.1621],\n",
              "        [0.6874],\n",
              "        [0.1029],\n",
              "        [0.0845],\n",
              "        [0.0503],\n",
              "        [0.2575],\n",
              "        [0.4046],\n",
              "        [0.2039],\n",
              "        [0.2886],\n",
              "        [0.0573],\n",
              "        [0.8073],\n",
              "        [0.3855],\n",
              "        [0.1845],\n",
              "        [0.6302],\n",
              "        [0.1261],\n",
              "        [0.8500],\n",
              "        [0.7875],\n",
              "        [0.0086],\n",
              "        [0.1075],\n",
              "        [0.1703],\n",
              "        [0.2293],\n",
              "        [0.5624],\n",
              "        [0.0056],\n",
              "        [0.1293],\n",
              "        [0.7283],\n",
              "        [0.3362],\n",
              "        [0.3010],\n",
              "        [0.0864],\n",
              "        [0.7027],\n",
              "        [0.5353],\n",
              "        [0.3067],\n",
              "        [0.1938],\n",
              "        [0.6063],\n",
              "        [0.6360],\n",
              "        [0.7219],\n",
              "        [0.6486],\n",
              "        [0.4237],\n",
              "        [0.1129],\n",
              "        [0.8183],\n",
              "        [0.4957],\n",
              "        [0.1677],\n",
              "        [0.9312],\n",
              "        [0.7788],\n",
              "        [0.1070],\n",
              "        [0.2503],\n",
              "        [0.3298],\n",
              "        [0.2114],\n",
              "        [0.7335],\n",
              "        [0.1433],\n",
              "        [0.9647],\n",
              "        [0.2933],\n",
              "        [0.7951],\n",
              "        [0.5170],\n",
              "        [0.2801],\n",
              "        [0.8339],\n",
              "        [0.1185],\n",
              "        [0.2355],\n",
              "        [0.5599],\n",
              "        [0.8966],\n",
              "        [0.2858],\n",
              "        [0.1955],\n",
              "        [0.1808],\n",
              "        [0.2796],\n",
              "        [0.3273],\n",
              "        [0.3835],\n",
              "        [0.2156],\n",
              "        [0.6563],\n",
              "        [0.5041],\n",
              "        [0.1733],\n",
              "        [0.2145],\n",
              "        [0.6059],\n",
              "        [0.4929],\n",
              "        [0.8539],\n",
              "        [0.4242],\n",
              "        [0.0949],\n",
              "        [0.1302],\n",
              "        [0.3532],\n",
              "        [0.3893],\n",
              "        [0.5571],\n",
              "        [0.3879],\n",
              "        [0.6845],\n",
              "        [0.3337],\n",
              "        [0.8670],\n",
              "        [0.6148],\n",
              "        [0.3463],\n",
              "        [0.9456],\n",
              "        [0.6276],\n",
              "        [0.1602],\n",
              "        [0.2217],\n",
              "        [0.8135],\n",
              "        [0.7284],\n",
              "        [0.8904],\n",
              "        [0.4616],\n",
              "        [0.8497],\n",
              "        [0.7486],\n",
              "        [0.6546],\n",
              "        [0.3843],\n",
              "        [0.9820],\n",
              "        [0.6012],\n",
              "        [0.3710],\n",
              "        [0.4929],\n",
              "        [0.9915],\n",
              "        [0.8358],\n",
              "        [0.4629],\n",
              "        [0.9902],\n",
              "        [0.7196],\n",
              "        [0.2338],\n",
              "        [0.0450],\n",
              "        [0.7906],\n",
              "        [0.9689],\n",
              "        [0.2087],\n",
              "        [0.7054],\n",
              "        [0.5213],\n",
              "        [0.3931],\n",
              "        [0.3640],\n",
              "        [0.6391],\n",
              "        [0.6102],\n",
              "        [0.8377],\n",
              "        [0.6830],\n",
              "        [0.7275],\n",
              "        [0.9411],\n",
              "        [0.6557],\n",
              "        [0.5742],\n",
              "        [0.3180],\n",
              "        [0.8667],\n",
              "        [0.5738],\n",
              "        [0.8223],\n",
              "        [0.9828],\n",
              "        [0.3882],\n",
              "        [0.8521],\n",
              "        [0.3985],\n",
              "        [0.9512],\n",
              "        [0.8935],\n",
              "        [0.3208],\n",
              "        [0.5179],\n",
              "        [0.2816],\n",
              "        [0.2978],\n",
              "        [0.3988],\n",
              "        [0.4966],\n",
              "        [0.3257],\n",
              "        [0.0280],\n",
              "        [0.1976],\n",
              "        [0.4573],\n",
              "        [0.3404],\n",
              "        [0.8714],\n",
              "        [0.9064],\n",
              "        [0.8185],\n",
              "        [0.7213],\n",
              "        [0.1036],\n",
              "        [0.9724],\n",
              "        [0.0594],\n",
              "        [0.3356],\n",
              "        [0.1481],\n",
              "        [0.6798],\n",
              "        [0.0839],\n",
              "        [0.2535],\n",
              "        [0.1824],\n",
              "        [0.1927],\n",
              "        [0.2666],\n",
              "        [0.9915],\n",
              "        [0.4592],\n",
              "        [0.6524],\n",
              "        [0.0037],\n",
              "        [0.9446],\n",
              "        [0.3911],\n",
              "        [0.0411],\n",
              "        [0.8446],\n",
              "        [0.3648],\n",
              "        [0.9541],\n",
              "        [0.1604],\n",
              "        [0.0358],\n",
              "        [0.1244],\n",
              "        [0.7166],\n",
              "        [0.4439],\n",
              "        [0.0368],\n",
              "        [0.5927],\n",
              "        [0.4681],\n",
              "        [0.6806],\n",
              "        [0.2457],\n",
              "        [0.1431],\n",
              "        [0.8408],\n",
              "        [0.2594],\n",
              "        [0.8517],\n",
              "        [0.9054],\n",
              "        [0.1503],\n",
              "        [0.2695],\n",
              "        [0.5602],\n",
              "        [0.0286],\n",
              "        [0.6536],\n",
              "        [0.2917],\n",
              "        [0.2783],\n",
              "        [0.1135],\n",
              "        [0.7339],\n",
              "        [0.1814],\n",
              "        [0.1003],\n",
              "        [0.3685],\n",
              "        [0.2216],\n",
              "        [0.0139],\n",
              "        [0.7015],\n",
              "        [0.3098],\n",
              "        [0.8502],\n",
              "        [0.8520],\n",
              "        [0.1811],\n",
              "        [0.2085],\n",
              "        [0.3225]])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Cx0wyYytSb7"
      },
      "source": [
        "## Problem 2: Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuNBMXGsO-7q"
      },
      "source": [
        "### Problem 2.1 Implementing and checking your gradients\n",
        "\n",
        "\n",
        "Write a function to compute the gradient of the average negative log likelihood and check your output against the results obtained by PyTorch's automatic differentiation functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROj5lRuOsASh"
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cl9CAUpTPtpw"
      },
      "source": [
        "### Problem 2.2: Implement Gradient Descent\n",
        "\n",
        "\n",
        "Now, use gradient descent to fit your Bradley-Terry model to the provided data.\n",
        "\n",
        "Deliverables for this question:\n",
        "1. Code the implements gradient descent to fit your Bradley-Terry model to the provided data.\n",
        "2. A plot of the loss curve of your algorithm and a brief discussion if it makes sense or not\n",
        "3. A plot of the histogram of the fitted values of $\\beta$\n",
        "4. The top 10 teams from your ranking, and a discussion of whether this ranking makes sense or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPSNWKE8sKIH"
      },
      "outputs": [],
      "source": [
        "# your code here (you can use multiple code and markdown cells to organize your answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBPDg-5QtXQV"
      },
      "source": [
        "## Problem 3: Newton's Method\n",
        "\n",
        "Now, use Newton's method to fit your Bradley-Terry model to the provided data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi_R1fgkFbQ0"
      },
      "source": [
        "### Problem 3.1 The Hessian\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS0kTKtVLDlQ"
      },
      "source": [
        "#### Problem 3.1.1. Implement and check the Hessian\n",
        "Write a function to compute the Hessian of the average negative log likelihood and check your answer against the output of `from torch.autograd.functional.hessian`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtSlxUAkLE-y"
      },
      "source": [
        "#### Problem 3.1.2: Positive definiteness\n",
        "\n",
        "Compute the Hessian at the point $\\beta = 0$ without regularization (set $\\gamma = 0$). Unless you've done sort of pre-processing, it's probably singular."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KQjQZtfsUZ6"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKVxT91XLbSL"
      },
      "source": [
        "#### Problem 3.1.3\n",
        "\n",
        "Describe intuitively and mathematically what it means for the Hessian of the negative log likelihood to be singular in the context of this data and model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAsLFSGXsWXO"
      },
      "source": [
        "*your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvClzEjJLk52"
      },
      "source": [
        "#### Problem 3.1.4\n",
        "\n",
        "Give a hypothesis for why the Hessian in this dataset and model is singular, and provide empirical evidence to support your hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFphHjnxsjE2"
      },
      "source": [
        "*your answer here*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wxWOBOQslRc"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtKlPKs9LyNw"
      },
      "source": [
        "#### Problem 3.1.5\n",
        "\n",
        "Explain why the Hessian is invertible when $\\gamma > 0$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgvigoXaspaw"
      },
      "source": [
        "*your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szaThYwMMuf4"
      },
      "source": [
        "### Problem 3.2: Implement Newton's method\n",
        "\n",
        "Now, use Newton's method to fit your $\\ell_2$-regularized Bradley-Terry model to the provided data.\n",
        "\n",
        "Deliverables for this question:\n",
        "1. Code the implements Newton's method to fit your Bradley-Terry model to the provided data.\n",
        "2. A plot of the loss curves from Newton's method and from gradient descent, using the same regularization strength $\\gamma$ and initialization $\\beta_0$. Briefly discuss the results and compare their rates of convergence.\n",
        "3. A plot of the histogram of the fitted values of $\\beta$\n",
        "4. The top 10 teams from your ranking, and a discussion of whether this ranking makes sense or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPYUCllcsri7"
      },
      "outputs": [],
      "source": [
        "# your code here (you can use multiple code and markdown cells to organize your answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9R91iI5NCMs"
      },
      "source": [
        "## Problem 4: Model criticism and revision\n",
        "\n",
        "Let's take another look the Bradley-Terry model from earlier and think about improvements we can make.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPSnL3odcj12"
      },
      "source": [
        "### Problem 4.1: Improvements to Bradley-Terry Model\n",
        "Choose one way to improve the Bradley-Terry model. Discuss *a priori* why you think this change will improve the model and implement your change."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gngpLxYpczp0"
      },
      "source": [
        "*your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xt9Yn0NPc3nS"
      },
      "source": [
        "### Problem 4.2: Evaluation\n",
        "Assess whether or not your change was an improvement or not. Provide empirical evidence by evaluating performance on a held out test set and include at least one plot supporting your assessment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQvtv-eHdBM5"
      },
      "source": [
        "*your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87F609vpdEq0"
      },
      "source": [
        "### Problem 4.3: Reflection\n",
        "Reflecting on the analysis we've conducted in this assignemnt, which conference is best? Is there a significant difference? Please justify your answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4YnbZWVdmWv"
      },
      "source": [
        "*your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TL_LAYoyI2T"
      },
      "source": [
        "## Submission Instructions\n",
        "\n",
        "**Formatting:** check that your code does not exceed 80 characters in line width. You can set _Tools &rarr; Settings &rarr; Editor &rarr; Vertical ruler column_ to 80 to see when you've exceeded the limit.\n",
        "\n",
        "**Converting to PDF** The simplest way to convert to PDF is to use the \"Print to PDF\" option in your browser. Just make sure that your code and plots aren't cut off, as it may not wrap lines.\n",
        "\n",
        "**Alternatively** You can download your notebook in .ipynb format and use the following commands to convert it to PDF.  Then run the following command to convert to a PDF:\n",
        "```\n",
        "jupyter nbconvert --to pdf <yourlastname>_hw<number>.ipynb\n",
        "```\n",
        "(Note that for the above code to work, you need to rename your file `<yourlastname>_hw<number>.ipynb`)\n",
        "\n",
        "**Installing nbconvert:**\n",
        "\n",
        "If you're using Anaconda for package management,\n",
        "```\n",
        "conda install -c anaconda nbconvert\n",
        "```\n",
        "\n",
        "**Upload** your .pdf file to Gradescope. Please tag your questions correctly! I.e., for each question, all of and only the relevant sections are tagged.\n",
        "\n",
        "Please post on Ed or come to OH if there are any other problems submitting the HW."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}